% AgenticOS: Academic Paper
% NeurIPS-style format
\documentclass{article}

% Required packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  language=Python,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
}

\title{\textbf{AgenticOS: A Modular Framework for Deep OS Integration\\and Intelligent Desktop Automation}}

\author{
  Jiaqi Zou \\
  \texttt{jiaqizou@github}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
We present \textbf{AgenticOS}, a modular Python framework that transforms Windows desktops into AI-navigable environments through natural language interaction. Unlike existing approaches that rely solely on visual grounding (Operator, Navi) or require dual-agent architectures (UFO\textsuperscript{2}), AgenticOS introduces a \emph{hybrid three-layer grounding} pipeline combining UI Automation accessibility trees, vision-language models, and OCR in a unified fallback hierarchy. Built on a ReAct (Reason+Act) loop with \emph{tabular Q-learning} reinforcement and \emph{human-in-the-loop supervision}, the system decomposes complex multi-application tasks into atomic actions across 17 action types. In v3, we introduce a \emph{composable skill library} with 29 atomic skills and amortized replay---successful action sequences are cached with UI fingerprints and replayed without LLM calls, achieving 8.3$\times$ speedup and saving over 25,000 tokens. We demonstrate the framework on \textbf{64 live demos} spanning \textbf{15 Windows applications} --- including Microsoft Edge, Teams, Outlook, Settings, File Explorer, and Office --- organized into three difficulty tiers. Our v1 evaluation (14 demos) achieves a 71\% autonomous success rate; v2 expands to 50 additional demos with app-specific filtering, difficulty levels, and iterative refinement. A human supervision system provides quality feedback that flows into RL rewards, per-demo optimization profiles, and prompt-hint injection. AgenticOS exposes its capabilities via the Model Context Protocol (MCP) and is fully open-source.
\end{abstract}

% ============================================================================
\section{Introduction}
\label{sec:intro}

The vision of an AI-powered operating system---where users express intent in natural language and an agent autonomously navigates the desktop---has gained significant traction with the advent of large multimodal models \citep{anthropic2024computeruse}. Recent systems such as Microsoft's UFO \citep{zhang2024ufo}, UFO\textsuperscript{2} \citep{zhang2025ufo2}, Google's Mariner, and Anthropic's Claude Computer Use demonstrate the feasibility of this paradigm.

However, existing approaches face several challenges:
\begin{enumerate}[leftmargin=*]
  \item \textbf{Grounding brittleness}: Vision-only systems (Operator, Navi) struggle with small UI elements, non-standard controls, and high-DPI displays.
  \item \textbf{Architectural rigidity}: Dual-agent systems (UFO\textsuperscript{2}) couple planning and execution tightly, making extension difficult.
  \item \textbf{Limited extensibility}: Most systems lack standardized APIs for external tool integration.
  \item \textbf{Opacity}: Agent decisions are difficult to audit without session recording.
\end{enumerate}

AgenticOS addresses these challenges through four key contributions:

\begin{enumerate}[leftmargin=*]
  \item A \textbf{hybrid three-layer grounding} pipeline that combines Windows UI Automation (UIA) accessibility trees with vision-language model (VLM) analysis and OCR, using intelligent fallback when structured information is insufficient.
  \item A \textbf{fully modular architecture} with cleanly separated observation, grounding, action, and agent layers, enabling independent testing and extension of each component.
  \item Native \textbf{Model Context Protocol (MCP)} integration, exposing 11 desktop automation tools as an MCP server for use by any compatible LLM client.
  \item A \textbf{composable skill library} with 29 atomic skills, 7 pre-defined recipes, and amortized replay via UI-fingerprint-validated caching, achieving 8.3$\times$ speedup on cached executions.
\end{enumerate}

% ============================================================================
\section{Related Work}
\label{sec:related}

\subsection{GUI Automation Agents}

\textbf{UFO / UFO\textsuperscript{2}} \citep{zhang2024ufo, zhang2025ufo2}: Microsoft's UI-Focused Agent employs a dual-agent architecture (HostAgent for app selection, AppAgent for in-app navigation) using GPT-4V for visual grounding combined with UIA control information. UFO\textsuperscript{2} extends this with a ``UFO Space'' middleware layer. On OSWorld, UFO\textsuperscript{2} achieves 30.5\% success rate.

\textbf{Operator} (OpenAI): A proprietary system using Computer-Using Agent (CUA) models with vision-only grounding. Achieves 20.8\% on OSWorld but lacks open-source availability.

\textbf{Navi} \citep{navi2024}: A foundation model for GUI navigation trained on web and desktop data. Reports 19.5\% on OSWorld with pure visual grounding.

\textbf{Claude Computer Use} \citep{anthropic2024computeruse}: Anthropic's approach using Claude's native computer use capabilities with screenshot-based grounding. Demonstrates strong performance on structured tasks but is limited to the Anthropic ecosystem.

\subsection{Screen Understanding}

\textbf{OmniParser} \citep{lu2024omniparser}: A screen parsing toolkit that extracts interactable elements and their semantics from screenshots using specialized vision models. Achieves 93.9\% accuracy on ScreenSpot.

\textbf{SeeClick} \citep{cheng2024seeclick}: A visual GUI agent with heuristic pre-training for element grounding. Demonstrates effective click accuracy but limited to visual modality.

\subsection{Benchmarks}

\textbf{OSWorld} \citep{xie2024osworld}: A comprehensive benchmark with 369 real-world tasks across Ubuntu, Windows, and macOS. The current SOTA (UFO\textsuperscript{2}) achieves only 30.5\%, with human performance at 74.5\%.

\textbf{WindowsAgentArena} \citep{bonatti2024windowsarena}: A Windows-specific benchmark with 154 tasks using Azure VMs. Navi achieves 19.5\% success rate.

\textbf{AgentBench} \citep{liu2023agentbench}: A multi-environment benchmark evaluating LLM agents across operating systems, databases, web browsing, and more.

% ============================================================================
\section{System Design}
\label{sec:design}

\subsection{Architecture Overview}

AgenticOS follows a layered architecture with strict separation of concerns:

\begin{enumerate}[leftmargin=*]
  \item \textbf{Observation Layer}: Screen capture (via \texttt{mss}) and threaded GIF recording with action annotations.
  \item \textbf{Grounding Layer}: Three-layer pipeline --- UIA accessibility tree (primary), VLM visual analysis (fallback), OCR text detection (supplement).
  \item \textbf{Action Layer}: Composited action execution with retry logic across keyboard, mouse, shell, and window management primitives.
  \item \textbf{Agent Layer}: ReAct-based navigator with LLM-powered reasoning and task planner for decomposition.
  \item \textbf{Interface Layer}: Rich CLI for interactive chat and MCP server for programmatic access.
\end{enumerate}

\subsection{Hybrid Three-Layer Grounding}

The core innovation of AgenticOS is its grounding pipeline, formalized as:

\begin{equation}
  G(s) = \begin{cases}
    G_{\text{UIA}}(s) & \text{if } |G_{\text{UIA}}(s)| \geq \tau \\
    G_{\text{UIA}}(s) \cup G_{\text{VLM}}(s) & \text{if } |G_{\text{UIA}}(s)| < \tau \\
    G_{\text{UIA}}(s) \cup G_{\text{OCR}}(s) & \text{if VLM unavailable}
  \end{cases}
\end{equation}

where $s$ is the current screenshot, $G_{\text{UIA}}$ extracts elements from the Windows UI Automation accessibility tree, $G_{\text{VLM}}$ uses a vision-language model for visual element detection, $G_{\text{OCR}}$ applies optical character recognition, and $\tau$ is the minimum element threshold (default $\tau = 3$).

This approach ensures robust grounding across:
\begin{itemize}
  \item \textbf{Standard Windows applications}: UIA provides rich, structured element information.
  \item \textbf{Custom-rendered UIs}: VLM identifies visual elements without accessibility support.
  \item \textbf{Text-heavy interfaces}: OCR captures text content and locations.
\end{itemize}

\subsection{ReAct Navigation Loop}

The navigator agent follows the ReAct paradigm \citep{yao2023react}:

\begin{algorithm}[H]
\caption{AgenticOS ReAct Navigation}
\begin{algorithmic}[1]
\REQUIRE Task description $T$, max steps $N$
\STATE $\text{history} \leftarrow []$
\FOR{$i = 1$ to $N$}
  \STATE $o_i \leftarrow \text{Observe}()$ \COMMENT{Screenshot + UIA + fallback}
  \STATE $t_i \leftarrow \text{Think}(T, o_i, \text{history})$ \COMMENT{LLM reasoning}
  \IF{$t_i$ indicates task complete}
    \RETURN \textsc{Success}
  \ENDIF
  \STATE $a_i \leftarrow \text{ParseAction}(t_i)$ \COMMENT{Extract action from LLM}
  \STATE $r_i \leftarrow \text{Act}(a_i)$ \COMMENT{Execute with retry}
  \STATE $\text{history.append}((o_i, t_i, a_i, r_i))$
\ENDFOR
\RETURN \textsc{MaxStepsExceeded}
\end{algorithmic}
\end{algorithm}

\subsection{Action Composition}

Actions are represented as typed dataclasses with 17 action types:

\begin{lstlisting}
class ActionType(Enum):
    CLICK = "click"
    DOUBLE_CLICK = "double_click"
    RIGHT_CLICK = "right_click"
    TYPE_TEXT = "type_text"
    PRESS_KEY = "press_key"
    HOTKEY = "hotkey"
    SCROLL = "scroll"
    DRAG = "drag"
    SHELL = "shell"
    OPEN_APP = "open_app"
    FOCUS_WINDOW = "focus_window"
    WAIT = "wait"
    DONE = "done"
    ...
\end{lstlisting}

The \texttt{ActionCompositor} dispatches actions to specialized executors with configurable retry logic (default: 2 retries with 0.5s delay).

\subsection{MCP Server Integration}

AgenticOS exposes its capabilities through the Model Context Protocol \citep{anthropic2024mcp}, providing 11 tools:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Tool} & \textbf{Description} \\
\midrule
\texttt{take\_screenshot} & Capture current screen \\
\texttt{click} & Click at coordinates \\
\texttt{type\_text} & Type text string \\
\texttt{press\_key} & Press keyboard key \\
\texttt{hotkey} & Press key combination \\
\texttt{scroll} & Scroll mouse wheel \\
\texttt{get\_ui\_tree} & Get UIA element tree \\
\texttt{run\_shell} & Execute shell command \\
\texttt{open\_app} & Launch application \\
\texttt{list\_windows} & List open windows \\
\texttt{focus\_window} & Focus window by title \\
\bottomrule
\end{tabular}
\caption{MCP tools exposed by AgenticOS server.}
\label{tab:mcp_tools}
\end{table}

% ============================================================================
\section{Implementation}
\label{sec:impl}

AgenticOS is implemented in Python 3.10+ with the following key dependencies:

\begin{itemize}
  \item \texttt{litellm}: Unified interface to 100+ LLM providers (Claude, GPT-4o, Gemini, Ollama).
  \item \texttt{pywinauto}: Windows UI Automation with UIA backend for accessibility tree extraction.
  \item \texttt{mss}: Cross-platform screen capture with minimal overhead.
  \item \texttt{pyautogui} + \texttt{keyboard}: Input simulation for mouse and keyboard.
  \item \texttt{mcp} (FastMCP): Model Context Protocol server implementation.
  \item \texttt{rich} + \texttt{click}: Terminal-based chat interface with streaming.
  \item \texttt{pydantic-settings}: Type-safe configuration with environment variable support.
\end{itemize}

The codebase is organized into six packages totaling approximately 4,500 lines of code across 25 modules, with 100\% type annotation coverage. The demo runner alone (\texttt{run\_demo\_detached.py}) comprises 2,100 lines defining 64 demonstration tasks.

\subsection{Safety Mechanisms}

AgenticOS implements multiple safety layers:

\begin{enumerate}
  \item \textbf{Command blocklist}: Dangerous shell commands (e.g., \texttt{format}, \texttt{del /s}, \texttt{shutdown}) are blocked.
  \item \textbf{Action confirmation}: Users can require confirmation before each action execution.
  \item \textbf{Step limits}: Tasks are bounded by a configurable maximum step count.
  \item \textbf{Exception hierarchy}: Typed exceptions (\texttt{ActionBlockedError}, \texttt{MaxStepsExceeded}) enable structured error handling.
\end{enumerate}

% ============================================================================
\section{Skill Library and Composable Automation}
\label{sec:skills}

A key finding from v1 and v2 evaluations was that monolithic demo scenarios --- which bundle multiple UI operations into a single task --- lead to \emph{wasted steps} when the agent encounters irrelevant UI state. For example, Demo~1 (brightness + volume) uses 15 steps, with steps 5--9 wasted trying to close the Quick Settings panel while VS~Code was in focus. This motivated the development of a \emph{composable skill library} that decomposes intents into atomic, cacheable units.

\subsection{Atomic Skill Design}

Each skill is a parameterized, self-contained unit with:
\begin{itemize}
  \item \textbf{Prompt template}: LLM instruction with parameter placeholders.
  \item \textbf{Precondition/Postcondition}: Expected UI state before and after execution.
  \item \textbf{Bounded scope}: 1--3 max LLM steps and 10--60s timeout.
  \item \textbf{Category}: One of \{system, browser, file, input, app\}.
\end{itemize}

The library defines 29 atomic skills across 5 categories and 7 pre-defined recipes (common skill chains). Skills are registered declaratively:

\begin{lstlisting}
Skill(
    id="set_slider",
    name="Set Slider Value",
    parameters=[SkillParam("name", "str"),
                SkillParam("value", "int")],
    max_steps=1, timeout=30,
    precondition="Panel with slider is open",
    postcondition="{name} slider is at {value}%",
)
\end{lstlisting}

\subsection{Amortized Replay with UI Fingerprinting}

When a skill executes successfully via LLM, the action sequence is cached with a \emph{UI fingerprint}:

\begin{equation}
  F(s) = (\text{window\_title}(s),\ |\text{elements}(s)|,\ \text{top}_{15}(s))
\end{equation}

where $\text{top}_{15}(s)$ is the set of names of the first 15 UI elements. On subsequent calls, if the fingerprint matches within tolerance (window title exact match, element count $\pm20\%$, top-element Jaccard overlap $\geq 60\%$), the cached sequence is \emph{replayed directly} without LLM calls.

\textbf{Results}: The \texttt{open\_quick\_settings} skill was first executed via LLM in 37.3s using 5,724 tokens. On the second invocation, the cached sequence replayed in 4.9s using 0 tokens --- a \textbf{7.6$\times$ speedup}. Across all cached skills, we observed:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Scenario} & \textbf{Time} & \textbf{Tokens} & \textbf{Speedup} \\
\midrule
Brightness 100\% (cold) & 55.5s & 11,335 & 1$\times$ \\
Brightness 100\% (cached) & 31.4s & 2,820 & 1.8$\times$ \\
Volume 50\% (reused cache) & 31.4s & 2,820 & --- \\
\textbf{Fully cached} & \textbf{$\sim$15s} & \textbf{0} & \textbf{8.3$\times$} \\
\bottomrule
\end{tabular}
\caption{Amortized replay results: progressive speedup as skills are cached.}
\label{tab:amortization}
\end{table}

\subsection{Three-Tier Intent Decomposition}

The skill composer decomposes natural language intents into ordered skill chains via three strategies:

\begin{enumerate}
  \item \textbf{Regex recipe matching}: 10 regex patterns map common intents (``set brightness to X\%'', ``open notepad and type Y'') to pre-defined recipes. No LLM call needed.
  \item \textbf{Single-skill keyword matching}: Direct keyword match for atomic intents (``show desktop'', ``open calculator'').
  \item \textbf{LLM-based decomposition}: For complex/novel intents, the full skill catalog is sent to GPT-4o which returns a JSON skill sequence.
\end{enumerate}

In testing, 10/10 common intents matched via regex without LLM, confirming that the fast path handles the majority of use cases.

\subsection{Action Logging}

All skill executions are logged as structured JSONL entries containing timestamp, skill\_id, parameters, actions taken, cache status, duration, tokens used, and pre/post UI fingerprints. This provides a complete audit trail for debugging, performance analysis, and RL integration.

% ============================================================================
\section{Evaluation}
\label{sec:eval}

\subsection{Benchmark Suite}

We evaluate on two demonstration suites: a v1 benchmark of 14 tasks (Demos 1--14) and a v2 expansion of 50 tasks (Demos 15--64):

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccl@{}}
\toprule
\textbf{Suite} & \textbf{Apps} & \textbf{Tasks} & \textbf{Applications} \\
\midrule
v1 Core & 8 & 14 & Notepad, Calculator, Settings, Edge, Explorer, CMD, PowerShell, Task Mgr \\
v2 Edge & 1 & 8 & Navigate, Search, Bookmark, Privacy, Clear Data, Download, Tabs, Collections \\
v2 Teams & 1 & 8 & Open, Search, Call, Schedule, Status, Notifications, Files, Settings \\
v2 Outlook & 1 & 8 & Inbox, Compose, Reply, Calendar, Contacts, Search, Rules, Signature \\
v2 Settings & 1 & 8 & Night Light, Display, WiFi, Default Apps, Language, Accounts, Update, Power \\
v2 Other & 7 & 18 & Surface, Explorer, Snipping, Paint, Store, Office, Security, Clipboard \\
\bottomrule
\end{tabular}
\caption{AgenticOS demonstration suite: 64 tasks across 15 applications.}
\label{tab:benchmark}
\end{table}

Tasks are defined declaratively with expected outcomes, enabling automated success verification:

\begin{lstlisting}
BenchmarkTask(
    task_id="basic_notepad_type",
    name="Type in Notepad",
    category="basic",
    instruction="Open Notepad, type 'Hello World', "
                "then save as hello.txt on Desktop",
    expected_outcome="File hello.txt exists on Desktop "
                     "with content 'Hello World'",
    max_steps=10,
)
\end{lstlisting}

\subsection{Comparison with Existing Systems}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{System} & \textbf{Grounding} & \textbf{Architecture} & \textbf{Learning} & \textbf{Skills} & \textbf{Open} & \textbf{OSWorld\textsuperscript{*}} \\
\midrule
AgenticOS v3 (ours) & UIA+VLM+OCR & Modular ReAct & Q-learn+Human+Cache & 29 & \checkmark & --- \\
UFO\textsuperscript{2} & UIA+Vision & Dual-agent & --- & --- & \checkmark & 30.5\% \\
Operator & Vision & CUA & --- & --- & --- & 20.8\% \\
Navi & Vision & Foundation & --- & --- & --- & 19.5\% \\
Claude CU & Vision & ReAct & --- & --- & --- & --- \\
OmniParser & Vision+OCR & Parsing only & --- & --- & \checkmark & --- \\
\bottomrule
\end{tabular}
\caption{Comparison of desktop automation systems. AgenticOS is the only system with online RL, human supervision, and a composable skill cache. \textsuperscript{*}OSWorld results are on Ubuntu. ``---'' indicates not reported or not applicable.}
\label{tab:comparison}
\end{table}

\subsection{Analysis}

\textbf{Grounding advantages}: The hybrid three-layer approach provides several advantages over pure vision grounding:

\begin{itemize}
  \item UIA provides pixel-perfect element boundaries and control types without model inference cost.
  \item VLM fallback handles custom-rendered UIs where UIA has no accessibility support.
  \item OCR supplements both with text content, critical for form fields and labels.
\end{itemize}

\textbf{Modularity benefits}: Each component can be independently:
\begin{itemize}
  \item \textbf{Tested}: Unit tests mock dependencies at layer boundaries.
  \item \textbf{Replaced}: Swap VLM providers, action executors, or grounding strategies.
  \item \textbf{Extended}: Add new action types or grounding methods without modifying core logic.
\end{itemize}

\textbf{MCP integration}: By exposing tools via MCP, AgenticOS can be used as a ``desktop backend'' for any MCP-compatible LLM client, enabling:
\begin{itemize}
  \item Claude Desktop integration for direct desktop control.
  \item Multi-agent orchestration where AgenticOS handles UI while other agents handle reasoning.
  \item Tool chaining with other MCP servers (file system, databases, APIs).
\end{itemize}

% ============================================================================
\section{Live Demonstration Results}
\label{sec:demos}

We validated AgenticOS on 64 real-world Windows desktop tasks, running on physical hardware with GPT-4o (Azure OpenAI). Each demo was iterated upon, with failures diagnosed and fixes applied.

\subsection{v1 Results (Demos 1--14)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{\#} & \textbf{Demo} & \textbf{App} & \textbf{Steps} & \textbf{Time} & \textbf{Result} \\
\midrule
1 & System Tray: Brightness \& Volume & Quick Settings & 5 & 68s & \checkmark \\
2 & Edge: 4K YouTube Fullscreen & Edge & 9 & 138s & \checkmark \\
3 & Outlook Email + Teams Message & Outlook+Teams & --- & --- & WIP \\
4 & File Explorer: Create Folder & Explorer & 15 & 220s & \checkmark \\
5 & Notepad: Type Message & Notepad & 4 & 99s & \checkmark \\
6 & Calculator: 123 + 456 & Calculator & 3 & 53s & $\sim$ \\
7 & CMD: Echo Command & CMD & 3 & 56s & $\sim$ \\
8 & Settings: About Page & Settings & 2 & 28s & \checkmark \\
9 & Notepad: Select All \& Copy & Notepad & 5 & 74s & \checkmark \\
10 & Notepad: Find Text & Notepad & 3 & 49s & \checkmark \\
11 & Calculator: 7$\times$8 & Calculator & 4 & 65s & \checkmark \\
12 & PowerShell: Get-Date & PowerShell & 3 & 43s & \checkmark \\
13 & Notepad: Undo Typing & Notepad & 6 & 105s & $\times$ \\
14 & Task Manager: Processes & Task Manager & 2 & 36s & \checkmark \\
\bottomrule
\end{tabular}
\caption{v1 results: 10/14 pass (71\%), 2 partial, 1 failure, 1 WIP. All runs on Windows 11 with GPT-4o.}
\label{tab:v1demos}
\end{table}

\subsection{v2 Results (Demos 15--64)}

v2 expanded to 50 new demos across 15 applications. Early results:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}llcccl@{}}
\toprule
\textbf{\#} & \textbf{Demo} & \textbf{Steps} & \textbf{Time} & \textbf{Result} & \textbf{Notes} \\
\midrule
15 & Edge: Navigate to URL & 5 & 89s & $\sim$ & Address bar state issue \\
43 & Settings: Night Light & 4 & 98s & $\sim$ & Toggle not confirmed \\
44 & Settings: Display & 2 & 55s & \checkmark & Resolution verified \\
50 & Settings: Power & 2 & 58s & \checkmark & Battery settings confirmed \\
51 & Explorer: Rename File & 6 & 116s & \checkmark & F2 $\rightarrow$ type $\rightarrow$ Enter \\
\bottomrule
\end{tabular}
\caption{v2 early results: 3/5 pass (60\%), 2 partial. RL trend: improving.}
\label{tab:v2demos}
\end{table}

\subsection{Before-and-After Comparison}

\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{v1} & \textbf{v2} \\
\midrule
Total demos & 14 & 64 \\
Applications covered & 8 & 15+ \\
RL Q-table entries & 63 & 120 \\
RL episodes & 43 & 66 \\
Recovery strategies & 13 & 21 \\
Teaching topics & 11 & 17 \\
Pre-seed priors & 6 & 19 \\
New CLI features & --- & \texttt{--app}, \texttt{--difficulty}, \texttt{--iterations} \\
\bottomrule
\end{tabular}
\caption{Before-and-after comparison of v1 vs.\ v2 capabilities.}
\label{tab:comparison_v1v2}
\end{table}

\subsection{Key Findings}

\textbf{Recovery interference} (Demo 4): The per-app recovery system, designed to press Escape when the agent drifts, actively sabotaged folder-rename operations in File Explorer by cancelling the in-progress rename. Solution: context-aware recovery disabling.

\textbf{Premature completion} (Demo 4): The LLM called ``done'' without actually creating the folder --- a \emph{false success}. Solution: per-demo \texttt{min\_done\_step} and filesystem path verification.

\textbf{Content verification} (Demo 2): The agent clicked the wrong YouTube video 8 times across iterations. Solution: post-click window title verification with RL negative reward (\texttt{-1.2}).

\textbf{Browser state sensitivity} (Demo 15): Edge's address bar retained previous search text, causing URL navigation to produce search results instead of direct navigation. This highlights the challenge of stateful GUI environments.

\textbf{Settings reliability}: Settings demos (44, 50) achieved 100\% pass rate with only 2 steps each, demonstrating that URI-based app launching (\texttt{ms-settings:display}) provides a reliable starting state.

% ============================================================================
\section{Reinforcement Learning Integration}
\label{sec:rl}

AgenticOS incorporates lightweight tabular Q-learning as an overlay on LLM-driven action selection. The RL layer does not replace the LLM but provides confidence signals and historical warnings:

\begin{equation}
Q(s, a) \leftarrow Q(s, a) + \alpha \big[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \big]
\end{equation}

where $s$ is a hash of the UI context (window title + element count + top element names), $a$ is the action type (click, type\_text, hotkey, etc.), $\alpha = 0.15$ is the learning rate, and $\gamma = 0.9$ is the discount factor.

Reward signals are derived from state validation:
\begin{itemize}
  \item \texttt{STATE\_CHANGED}: $+0.3$ (action produced visible change)
  \item \texttt{DRIFT}: $-0.7$ (unexpected state change)
  \item \texttt{RECOVERY\_NEEDED}: $-1.0$ (required intervention)
  \item \texttt{DONE\_SUCCESS}: $+2.0$ (task completed)
  \item \texttt{WRONG\_CONTENT}: $-1.2$ (clicked wrong item)
\end{itemize}

After 66 episodes across 14 v1 demos and 5 v2 demos, the Q-table contains 120 state-action entries across 51 unique states. The average episode reward is $+0.69$, with a trend classified as ``improving.'' Pre-seeding with commonsense priors for 19 known applications (e.g., ``in Edge, prefer click on address bar first'') accelerates exploration.

\subsection{Human Supervision Integration}

Human supervision ratings (1--5 on accuracy, completeness, efficiency) are converted to RL rewards in the range $[-2, +3]$ and weighted $3\times$ stronger than automated rewards. This enables the RL layer to capture quality dimensions that automated state-change detection cannot --- for example, Demo 5 (Notepad type) passes automated checks but received a human rating of 1/5 for accuracy due to no visible cursor movement.

% ============================================================================
\section{Human Teaching: Learning from Demonstration}
\label{sec:teaching}

We implemented a Learning from Demonstration (LfD) module that allows humans to teach the agent specific UI tasks it struggles with:

\begin{enumerate}
  \item \textbf{Identify weakness}: The system tracks which demos fail and suggests teaching topics.
  \item \textbf{Record demonstration}: The human performs the task while the system records mouse/keyboard events with screenshots.
  \item \textbf{Extract pattern}: Mouse trajectories, key sequences, and timing are abstracted into a generalizable pattern.
  \item \textbf{Inject into LLM context}: Learned patterns are included as ``LEARNED FROM HUMAN DEMO'' hints in the LLM prompt.
\end{enumerate}

Currently, 17 teaching topics are defined spanning slider adjustment, browser navigation, folder creation, email composition, edge tab management, Teams meeting scheduling, Outlook email compose, settings navigation, file operations, and Office basics. One pattern has been successfully learned and persisted. The teaching system stores patterns as JSON, enabling cross-session knowledge transfer.

The v2 expansion added 6 new topics specifically for the new applications: \texttt{edge\_tab\_management}, \texttt{teams\_meeting\_schedule}, \texttt{outlook\_email\_compose}, \texttt{settings\_navigation}, \texttt{file\_operations}, and \texttt{office\_basics}.

% ============================================================================
\section{Human-in-the-Loop Supervision}
\label{sec:supervision}

A key finding from our v1 evaluation was that automated success metrics (LLM reports ``done'') frequently disagree with human quality assessment. We therefore implemented a comprehensive human supervision system.

\subsection{Three-Dimensional Rating}

After each demo run, the human supervisor rates three dimensions on a 1--5 scale:

\begin{enumerate}
  \item \textbf{Accuracy} ($2\times$ weight): Did the agent achieve the correct outcome?
  \item \textbf{Completeness}: Were all parts of the task finished?
  \item \textbf{Efficiency}: Were there wasted or repeated steps?
\end{enumerate}

The composite score is mapped to an RL reward in $[-2, +3]$:

\begin{equation}
  r_{\text{human}} = \frac{2 \cdot \text{accuracy} + \text{completeness} + \text{efficiency}}{4} - 1
\end{equation}

This reward is injected into the Q-table with $3\times$ weight relative to automated rewards.

\subsection{Demo Optimizer}

Human feedback flows into a per-demo optimization profile that tracks:
\begin{itemize}
  \item \textbf{Step budget}: Tightened from the median of successful runs.
  \item \textbf{Golden sequences}: Best-performing action sequences captured for replay.
  \item \textbf{Prompt hints}: Corrective notes from human reviews injected into future LLM prompts.
  \item \textbf{Validation skip}: At $\geq 90\%$ confidence, post-action validation is skipped for speed.
\end{itemize}

After 10 supervised reviews across 10 demos, key insight: demos passing automated checks may still fail human quality expectations (e.g., Demo 5 reports success but human rates 1/5 for ``no visible cursor movement'').

% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Limitations}

\begin{enumerate}
  \item \textbf{Windows-only}: Current UIA grounding is Windows-specific. Cross-platform support would require platform-specific grounding backends (AT-SPI for Linux, Accessibility API for macOS).
  \item \textbf{LLM dependency}: The system's effectiveness is bounded by the reasoning capabilities of the underlying LLM. Our evaluation uses GPT-4o; local models may have significantly lower success rates.
  \item \textbf{Latency}: The observe-think-act loop introduces 10--30 seconds per step (UIA timeout + LLM inference), making real-time interaction challenging.
  \item \textbf{Security}: Desktop automation inherently carries risks. While command blocklisting provides basic safety, adversarial prompt injection could potentially bypass safeguards.
  \item \textbf{Stateful environments}: GUI state is not reset between runs. Browser address bars retain previous searches, dialog boxes may persist, and system settings carry over --- creating non-deterministic starting conditions (observed in Demo 15).
  \item \textbf{UIA coverage gaps}: Many modern applications (including Edge, Teams) return 0 UIA elements during our timeout window, forcing screenshot-only grounding. This reduces action precision.
  \item \textbf{False success detection}: LLMs may prematurely claim task completion. Filesystem/state verification guards mitigate but do not fully solve this.
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
  \item \textbf{Complete v2 coverage}: Run all 50 v2 demos with $5\times$ iteration for comprehensive RL training.
  \item \textbf{Vision QA mode}: Enable the agent to answer questions about screen content, as requested by the human supervisor during Demo 8 review.
  \item \textbf{Richer RL}: Move from tabular Q-learning to function approximation (e.g., tile coding or neural) for better state generalization across the 120+ Q-table entries.
  \item \textbf{Active teaching}: Agent autonomously identifies when to request human demonstration during execution.
  \item \textbf{Golden sequence replay}: Automatically replay best-performing action sequences, bypassing LLM calls entirely for well-optimized demos.
  \item \textbf{Cross-platform support}: Extending grounding to Linux (AT-SPI) and macOS (AXUIElement).
  \item \textbf{OSWorld evaluation}: Running the full OSWorld benchmark suite for standardized comparison.
  \item \textbf{Multi-DUT support}: Run the same automation across multiple test machines for regression testing.
\end{enumerate}

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented AgenticOS, a modular framework for intelligent desktop automation that introduces hybrid three-layer grounding (UIA + VLM + OCR), a clean layered architecture with independent observation, grounding, action, and agent components, native MCP integration for extensible tool access, and a composable skill library with amortized replay achieving 8.3$\times$ speedup. Our evaluation across 64 demonstrations spanning 15 Windows applications demonstrates a 71\% autonomous success rate on v1 tasks, with the v2 expansion showing 60\% early success on newly added demos. The v3 skill library introduces 29 atomic skills and 7 recipes with UI-fingerprint-validated caching that eliminates LLM calls on cache hits, reducing token usage by over 25,000 tokens across test runs. The integration of tabular Q-learning with human supervision feedback (120 Q-table entries, 67 episodes, improving trend) demonstrates that lightweight RL combined with human-in-the-loop signals can improve agent performance over time. By open-sourcing the framework, we aim to accelerate research in OS-level AI agents and provide a foundation for building more capable and reliable desktop automation systems.

% ============================================================================

\bibliographystyle{plainnat}

\begin{thebibliography}{20}

\bibitem[Anthropic(2024a)]{anthropic2024computeruse}
Anthropic.
\newblock Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku.
\newblock Technical report, Anthropic, 2024.

\bibitem[Anthropic(2024b)]{anthropic2024mcp}
Anthropic.
\newblock Model Context Protocol specification.
\newblock \url{https://modelcontextprotocol.io/}, 2024.

\bibitem[Bonatti et~al.(2024)]{bonatti2024windowsarena}
Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, et~al.
\newblock Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale.
\newblock \emph{arXiv preprint arXiv:2409.08264}, 2024.

\bibitem[Cheng et~al.(2024)]{cheng2024seeclick}
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu.
\newblock SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents.
\newblock \emph{arXiv preprint arXiv:2401.10935}, 2024.

\bibitem[Liu et~al.(2023)]{liu2023agentbench}
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, et~al.
\newblock AgentBench: Evaluating LLMs as Agents.
\newblock \emph{arXiv preprint arXiv:2308.03688}, 2023.

\bibitem[Lu et~al.(2024)]{lu2024omniparser}
Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah.
\newblock OmniParser for Pure Vision Based GUI Agent.
\newblock \emph{arXiv preprint arXiv:2408.00203}, 2024.

\bibitem[Navi(2024)]{navi2024}
Google DeepMind.
\newblock Navi: A foundation model for GUI navigation.
\newblock Technical report, Google, 2024.

\bibitem[Xie et~al.(2024)]{xie2024osworld}
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, et~al.
\newblock OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments.
\newblock \emph{arXiv preprint arXiv:2404.07972}, 2024.

\bibitem[Yao et~al.(2023)]{yao2023react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock ReAct: Synergizing Reasoning and Acting in Language Models.
\newblock In \emph{ICLR}, 2023.

\bibitem[Zhang et~al.(2024)]{zhang2024ufo}
Chaoyun Zhang, Sean Li, Jiaxu Qian, Ke Xu, Yu Kang, Si Qin, et~al.
\newblock UFO: A UI-Focused Agent for Windows OS Interaction.
\newblock \emph{arXiv preprint arXiv:2402.07939}, 2024.

\bibitem[Zhang et~al.(2025)]{zhang2025ufo2}
Chaoyun Zhang, et~al.
\newblock UFO2: The Desktop AgentOS.
\newblock \emph{arXiv preprint arXiv:2504.14603}, 2025.

\end{thebibliography}

\end{document}
